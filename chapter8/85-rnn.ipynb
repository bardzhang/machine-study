{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f8c80536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the time machine by h g wells\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. 读取文件\n",
    "# 2. 词元化 将文文本数据拆分成字符或者单词\n",
    "# 3. 建立词表  词表包含单词或者字符的频率， 从id到单词或者字符的索引 数组表示，从单词或者字符的索引到单词 字典表示\n",
    "#              文章的长度__len__  __getitem__ 返回token所在的索引  根据索引返回对应的单词或者字符 用<unk>表示空\n",
    "\n",
    "import re \n",
    "\n",
    "def readfile():\n",
    "    lines=[]\n",
    "    with open(\"../data/timemachine.txt\") as f:\n",
    "        for line in f:\n",
    "            lines.append(re.sub(\"[^A-Za-z]+\",' ',line).strip().lower())\n",
    "    return lines\n",
    "\n",
    "lines=readfile()\n",
    "for i in range(2):\n",
    "    print(lines[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "af537fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'time', 'machine', 'by', 'h', 'g', 'wells']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['i']\n",
      "[]\n",
      "[]\n",
      "['the', 'time', 'traveller', 'for', 'so', 'it', 'will', 'be', 'convenient', 'to', 'speak', 'of', 'him']\n",
      "['was', 'expounding', 'a', 'recondite', 'matter', 'to', 'us', 'his', 'grey', 'eyes', 'shone', 'and']\n",
      "['twinkled', 'and', 'his', 'usually', 'pale', 'face', 'was', 'flushed', 'and', 'animated', 'the']\n"
     ]
    }
   ],
   "source": [
    "def tokenize(lines,type='word'):\n",
    "    if type=='word':\n",
    "        return [ line.split() for line in lines]\n",
    "    elif type=='char':\n",
    "        return [list(line) for line in lines]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "tokens=tokenize(lines)\n",
    "for i in range(11):\n",
    "    print(tokens[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5194cb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 2261), ('i', 1267), ('and', 1245), ('of', 1155), ('a', 816), ('to', 695), ('was', 552), ('in', 541), ('that', 443), ('my', 440)]\n",
      "[(0, '<unk>'), ('the', 1), ('i', 2), ('and', 3), ('of', 4), ('a', 5), ('to', 6), ('was', 7), ('in', 8), ('that', 9)]\n",
      "文本： ['the', 'time', 'machine', 'by', 'h', 'g', 'wells']\n",
      "索引： [1, 19, 50, 40, 2183, 2184, 400]\n",
      "文本： []\n",
      "索引： []\n",
      "文本： []\n",
      "索引： []\n",
      "文本： []\n",
      "索引： []\n",
      "文本： []\n",
      "索引： []\n",
      "文本： ['i']\n",
      "索引： [2]\n",
      "文本： []\n",
      "索引： []\n",
      "文本： []\n",
      "索引： []\n",
      "文本： ['the', 'time', 'traveller', 'for', 'so', 'it', 'will', 'be', 'convenient', 'to', 'speak', 'of', 'him']\n",
      "索引： [1, 19, 71, 16, 37, 11, 115, 42, 680, 6, 586, 4, 108]\n",
      "文本： ['was', 'expounding', 'a', 'recondite', 'matter', 'to', 'us', 'his', 'grey', 'eyes', 'shone', 'and']\n",
      "索引： [7, 1420, 5, 2185, 587, 6, 126, 25, 330, 127, 439, 3]\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "class Vocab():\n",
    "    def __init__(self,tokens):\n",
    "        words=[word for token in tokens for word in token]\n",
    "        counter=collections.Counter(words)\n",
    "        self._token_freq=sorted(counter.items(),key=lambda x: x[1],reverse=True)\n",
    "        \n",
    "        self.idx_to_token=['<unk>']\n",
    "        self.token_to_idx={0:'<unk>'}\n",
    "        for token,freq in self._token_freq:\n",
    "            if token not in self.token_to_idx:\n",
    "                self.idx_to_token.append(token)\n",
    "                self.token_to_idx[token]=len(self.idx_to_token)-1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "    \n",
    "    def __getitem__(self,tokens):\n",
    "        return [self.token_to_idx.get(token,self.unk) for token in tokens]\n",
    "\n",
    "    def to_tokens(self,indices):\n",
    "        return [ self.idx_to_token[index] for index in indices]\n",
    "\n",
    "    @property\n",
    "    def unk(self):\n",
    "        return 0\n",
    "\n",
    "    @property\n",
    "    def token_freq(self):\n",
    "        return self._token_freq\n",
    "    \n",
    "vocab=Vocab(tokens)\n",
    "print(vocab.token_freq[:10])\n",
    "print(list(vocab.token_to_idx.items())[:10])\n",
    "for i in range(10):\n",
    "    print(\"文本：\",tokens[i])\n",
    "    print(\"索引：\",vocab[tokens[i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "13273019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(170580, 28)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_corpus_time_machine():\n",
    "    lines=readfile()\n",
    "    tokens=tokenize(lines,'char')\n",
    "    vocab=Vocab(tokens)\n",
    "\n",
    "    corpus=[ch for token in tokens for ch in token]\n",
    "    return corpus,vocab\n",
    "\n",
    "corpus,vocab=load_corpus_time_machine()\n",
    "len(corpus),len(vocab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
