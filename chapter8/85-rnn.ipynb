{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8c80536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the time machine by h g wells\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. 读取文件\n",
    "# 2. 词元化 将文文本数据拆分成字符或者单词\n",
    "# 3. 建立词表  词表包含单词或者字符的频率， 从id到单词或者字符的索引 数组表示，从单词或者字符的索引到单词 字典表示\n",
    "#              文章的长度__len__  __getitem__ 返回token所在的索引  根据索引返回对应的单词或者字符 用<unk>表示空\n",
    "\n",
    "import re \n",
    "\n",
    "def readfile():\n",
    "    lines=[]\n",
    "    with open(\"../data/timemachine.txt\") as f:\n",
    "        for line in f:\n",
    "            lines.append(re.sub(\"[^A-Za-z]+\",' ',line).strip().lower())\n",
    "    return lines\n",
    "\n",
    "lines=readfile()\n",
    "for i in range(2):\n",
    "    print(lines[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af537fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'time', 'machine', 'by', 'h', 'g', 'wells']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['i']\n",
      "[]\n",
      "[]\n",
      "['the', 'time', 'traveller', 'for', 'so', 'it', 'will', 'be', 'convenient', 'to', 'speak', 'of', 'him']\n",
      "['was', 'expounding', 'a', 'recondite', 'matter', 'to', 'us', 'his', 'grey', 'eyes', 'shone', 'and']\n",
      "['twinkled', 'and', 'his', 'usually', 'pale', 'face', 'was', 'flushed', 'and', 'animated', 'the']\n"
     ]
    }
   ],
   "source": [
    "def tokenize(lines,type='word'):\n",
    "    if type=='word':\n",
    "        return [ line.split() for line in lines]\n",
    "    elif type=='char':\n",
    "        return [list(line) for line in lines]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "tokens=tokenize(lines)\n",
    "for i in range(11):\n",
    "    print(tokens[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5194cb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 2261), ('i', 1267), ('and', 1245), ('of', 1155), ('a', 816), ('to', 695), ('was', 552), ('in', 541), ('that', 443), ('my', 440)]\n",
      "[(0, '<unk>'), ('the', 1), ('i', 2), ('and', 3), ('of', 4), ('a', 5), ('to', 6), ('was', 7), ('in', 8), ('that', 9)]\n",
      "文本： ['the', 'time', 'machine', 'by', 'h', 'g', 'wells']\n",
      "索引： [1, 19, 50, 40, 2183, 2184, 400]\n",
      "文本： []\n",
      "索引： []\n",
      "文本： []\n",
      "索引： []\n",
      "文本： []\n",
      "索引： []\n",
      "文本： []\n",
      "索引： []\n",
      "文本： ['i']\n",
      "索引： [2]\n",
      "文本： []\n",
      "索引： []\n",
      "文本： []\n",
      "索引： []\n",
      "文本： ['the', 'time', 'traveller', 'for', 'so', 'it', 'will', 'be', 'convenient', 'to', 'speak', 'of', 'him']\n",
      "索引： [1, 19, 71, 16, 37, 11, 115, 42, 680, 6, 586, 4, 108]\n",
      "文本： ['was', 'expounding', 'a', 'recondite', 'matter', 'to', 'us', 'his', 'grey', 'eyes', 'shone', 'and']\n",
      "索引： [7, 1420, 5, 2185, 587, 6, 126, 25, 330, 127, 439, 3]\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "class Vocab():\n",
    "    def __init__(self,tokens):\n",
    "        words=[word for token in tokens for word in token]\n",
    "        counter=collections.Counter(words)\n",
    "        self._token_freq=sorted(counter.items(),key=lambda x: x[1],reverse=True)\n",
    "        \n",
    "        self.idx_to_token=['<unk>']\n",
    "        self.token_to_idx={0:'<unk>'}\n",
    "        for token,freq in self._token_freq:\n",
    "            if token not in self.token_to_idx:\n",
    "                self.idx_to_token.append(token)\n",
    "                self.token_to_idx[token]=len(self.idx_to_token)-1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "    \n",
    "    def __getitem__(self,tokens):\n",
    "        return [self.token_to_idx.get(token,self.unk) for token in tokens]\n",
    "\n",
    "    def to_tokens(self,indices):\n",
    "        return [ self.idx_to_token[index] for index in indices]\n",
    "\n",
    "    @property\n",
    "    def unk(self):\n",
    "        return 0\n",
    "\n",
    "    @property\n",
    "    def token_freq(self):\n",
    "        return self._token_freq\n",
    "    \n",
    "vocab=Vocab(tokens)\n",
    "print(vocab.token_freq[:10])\n",
    "print(list(vocab.token_to_idx.items())[:10])\n",
    "for i in range(10):\n",
    "    print(\"文本：\",tokens[i])\n",
    "    print(\"索引：\",vocab[tokens[i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "13273019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(170580, 28)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_corpus_time_machine():\n",
    "    lines=readfile()\n",
    "    tokens=tokenize(lines,'char')\n",
    "    vocab=Vocab(tokens)\n",
    "\n",
    "    corpus=[vocab[ch] for token in tokens for ch in token]\n",
    "    return corpus,vocab\n",
    "\n",
    "corpus,vocab=load_corpus_time_machine()\n",
    "len(corpus),len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "63aba7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[26, 27, 28, 29, 30],\n",
      "        [ 1,  2,  3,  4,  5]]) \n",
      " tensor([[27, 28, 29, 30, 31],\n",
      "        [ 2,  3,  4,  5,  6]])\n",
      "tensor([[16, 17, 18, 19, 20],\n",
      "        [ 6,  7,  8,  9, 10]]) \n",
      " tensor([[17, 18, 19, 20, 21],\n",
      "        [ 7,  8,  9, 10, 11]])\n",
      "tensor([[21, 22, 23, 24, 25],\n",
      "        [11, 12, 13, 14, 15]]) \n",
      " tensor([[22, 23, 24, 25, 26],\n",
      "        [12, 13, 14, 15, 16]])\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "\n",
    "def seq_data_iter_random(corpus,batch_size,num_steps):\n",
    "\n",
    "    corpus=corpus[random.randint(0,num_steps-1):]\n",
    "\n",
    "    num_subseq=(len(corpus)-1) // num_steps\n",
    "\n",
    "    seq_indices=list(range(0,num_subseq*num_steps,num_steps))\n",
    "\n",
    "    random.shuffle(seq_indices)\n",
    "\n",
    "    def data(pos):\n",
    "        return corpus[pos:pos+num_steps]\n",
    "\n",
    "    num_batch= len(seq_indices) // batch_size\n",
    "    for i in range(0,num_batch*batch_size,batch_size):\n",
    "\n",
    "        seq_sub_indices=seq_indices[i:i+batch_size]\n",
    "        X=[data(j) for j in seq_sub_indices]\n",
    "        Y=[data(j+1) for j in seq_sub_indices]\n",
    "        yield torch.tensor(X),torch.tensor(Y)\n",
    "\n",
    "    \n",
    "myseq=list(range(35))\n",
    "data_iter=seq_data_iter_random(myseq,2,5)\n",
    "for x,y in data_iter:\n",
    "    print(x,'\\n',y)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ed5cbe06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3,  4,  5,  6,  7],\n",
      "        [18, 19, 20, 21, 22]]) \n",
      " tensor([[ 4,  5,  6,  7,  8],\n",
      "        [19, 20, 21, 22, 23]])\n",
      "tensor([[ 8,  9, 10, 11, 12],\n",
      "        [23, 24, 25, 26, 27]]) \n",
      " tensor([[ 9, 10, 11, 12, 13],\n",
      "        [24, 25, 26, 27, 28]])\n",
      "tensor([[13, 14, 15, 16, 17],\n",
      "        [28, 29, 30, 31, 32]]) \n",
      " tensor([[14, 15, 16, 17, 18],\n",
      "        [29, 30, 31, 32, 33]])\n"
     ]
    }
   ],
   "source": [
    "def seq_data_iter_seq(corpus,batch_size,num_steps):\n",
    "    offset=random.randint(0,num_steps)\n",
    "    num_total=((len(corpus)-offset-1)//batch_size)*batch_size\n",
    "    xs=torch.tensor(corpus[offset:num_total+offset])\n",
    "    ys=torch.tensor(corpus[offset+1:num_total+offset+1])\n",
    "    xs,ys=xs.reshape(batch_size,-1),ys.reshape(batch_size,-1)\n",
    "    num_batch=xs.shape[1]//num_steps\n",
    "    for i in range(0,num_batch*num_steps,num_steps):\n",
    "        x=xs[:,i:i+num_steps]\n",
    "        y=ys[:,i:i+num_steps]\n",
    "        yield x,y\n",
    "\n",
    "data_iter=seq_data_iter_seq(myseq,2,5)\n",
    "for x,y in data_iter:\n",
    "    print(x,'\\n',y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "29243a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3, 9, 2, 1, 3],\n",
      "        [3, 9, 4, 3, 1]]) \n",
      " tensor([[9, 2, 1, 3, 5],\n",
      "        [9, 4, 3, 1, 3]])\n",
      "tensor([[ 5, 13,  2,  1, 13],\n",
      "        [ 3,  9,  5,  8,  1]]) \n",
      " tensor([[13,  2,  1, 13,  4],\n",
      "        [ 9,  5,  8,  1, 21]])\n",
      "tensor([[ 4, 15,  9,  5,  6],\n",
      "        [21, 12,  2,  4, 15]]) \n",
      " tensor([[15,  9,  5,  6,  2],\n",
      "        [12,  2,  4, 15,  9]])\n",
      "tensor([[ 2,  1, 21, 19,  1],\n",
      "        [ 9,  2, 11,  7, 21]]) \n",
      " tensor([[ 1, 21, 19,  1,  9],\n",
      "        [ 2, 11,  7, 21,  8]])\n",
      "tensor([[ 9,  1, 18,  1, 17],\n",
      "        [ 8, 15,  2,  6,  2]]) \n",
      " tensor([[ 1, 18,  1, 17,  2],\n",
      "        [15,  2,  6,  2,  1]])\n"
     ]
    }
   ],
   "source": [
    "def load_data_time_machine(batch_size,num_steps,use_random_iter=False):\n",
    "    corpus,vocab=load_corpus_time_machine()\n",
    "    if use_random_iter:\n",
    "        return seq_data_iter_random(corpus,batch_size,num_steps),vocab\n",
    "    else:\n",
    "        return seq_data_iter_seq(corpus,batch_size,num_steps),vocab\n",
    "\n",
    "data_iter,vocab=load_data_time_machine(2,5)\n",
    "i=0\n",
    "for x,y in data_iter:\n",
    "    if i==5:\n",
    "        break\n",
    "    else:\n",
    "        print(x,'\\n',y)\n",
    "        i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "abf134fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2, 3, 4],\n",
      "        [5, 6, 7, 8, 9]])\n",
      "torch.Size([5, 2, 28])\n",
      "tensor([[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0]],\n",
      "\n",
      "        [[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "          0, 0, 0, 0, 0]]])\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "X=torch.arange(10).reshape((2,5))\n",
    "print(X)\n",
    "test=F.one_hot(X.T,len(vocab))\n",
    "print(test.shape)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1087a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 初始化模型参数 x 2*28  w_xh 28*255 w_hh 255*255 w_b 255  h_t 2*255   w_hq 255*28  b_q 28 \n",
    "#2 初始化 h_0 参数  \n",
    "#3 实现模型 隐藏层255 词大小28 h_t=tanh(x*w_xh+h_t-1*w_hh+w_b)  o=h_t*W_hq+b_q  \n",
    "   # 原本x是批次大小*时间序列  转换成时间序列 批次大小 词表大小（独热编码的形式），的数据 在通过循环时间序列计算，最终将结果组合并拼接\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a57b0e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0,  1,  2,  3],\n",
      "         [ 4,  5,  6,  7],\n",
      "         [ 8,  9, 10, 11]],\n",
      "\n",
      "        [[12, 13, 14, 15],\n",
      "         [16, 17, 18, 19],\n",
      "         [20, 21, 22, 23]]])\n",
      "tensor([[[ 0,  1,  2,  3],\n",
      "         [ 4,  5,  6,  7],\n",
      "         [ 8,  9, 10, 11]],\n",
      "\n",
      "        [[12, 13, 14, 15],\n",
      "         [16, 17, 18, 19],\n",
      "         [20, 21, 22, 23]]])\n",
      "tensor([[[ 0,  1,  2,  3,  0,  1,  2,  3],\n",
      "         [ 4,  5,  6,  7,  4,  5,  6,  7],\n",
      "         [ 8,  9, 10, 11,  8,  9, 10, 11]],\n",
      "\n",
      "        [[12, 13, 14, 15, 12, 13, 14, 15],\n",
      "         [16, 17, 18, 19, 16, 17, 18, 19],\n",
      "         [20, 21, 22, 23, 20, 21, 22, 23]]])\n"
     ]
    }
   ],
   "source": [
    "a=torch.arange(24).reshape((2,3,4))\n",
    "print(a)\n",
    "b=torch.arange(24).reshape((2,3,4))\n",
    "print(b)\n",
    "c=torch.cat([a,b],dim=2)\n",
    "print(c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
